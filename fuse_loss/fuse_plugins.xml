<library path="lib/libfuse_loss">
  <!--
  Loss functions are M-estimators that take the squared norm of a residual block 'r' and try to reduce the effect of
  outliers by replacing the squared residuals 's = r^2' used in the standard Least-Squares problem by another function
  of the squared residuals `rho(s)`.

  For this reason loss functions are also known as robustifiers.

  In the notation used here, `rho(s)` takes the squared residuals `s` as an input, not the residuals `r`, as in Ceres
  solver. More precisely, `s` is the squared norm of the residuals:

    s = ||residuals||^2

  but for simplicity we refer to it as the squared residual.

  Similarly, the `rho(s)` equations are multiplied by `2` because the convention in Ceres solver is that the
  contribution of a term to the cost function is given by `1/2 rho(s)`.

  For least squares problems where there are no outliers and standard squared loss is expected, it is not necessary to
  create use a loss function.

  For least squares problems where the optimization may encounter input terms that contain outliers, i.e. completely
  bogus measurements, it is important to use a loss function that reduces their associated penalty cost.

  Using a robust loss function, the cost for large residuals is reduced. This leads to outlier terms getting
  downweighted so they do not overly influence the final solution of the problem.

  Unfortunately, there is no principled way to select a robust loss function. It is encouraged to start with a
  non-robust cost, then only experiment with robust loss functions if standard squared loss does not work.

  See http://ceres-solver.org/nnls_modeling.html#lossfunction and
  https://github.com/ceres-solver/ceres-solver/blob/master/include/ceres/loss_function.h for more information. Indeed,
  the last paragraphs above have been either copied or adapted from there in order to give some basic guidelines.

  Depending on how much the outliers are downweighted, the loss functions can be more or less aggressive. The more
  aggressive ones would give a lower cost after applying the loss function. Therefore, we can order the loss functions
  according to this criterion, starting from the Trivial loss, which is equivalent to the standard squared loss, i.e. no
  loss (aka non-robust cost):

    Trivial >= Huber >= SoftLOne >= Fair >= Cauchy >= DCS >= Arctan >= GemanMcClure >= Welsch >= Tukey

  The Tolerant loss is more like the TrivialLoss, expect for the deadband for residuals with low cost. This is the only
  loss function that downweights the residuals with small cost, as opossed to all other loss functions, that downweight
  the residuals with large cost.

  The test_qwt_loss_plot generates a set of plots that show how the \rho, influence (\phi) and weight functions look
  like for all the loss functions. The order used to compared them above is based on the weight function, and it is
  orientative. It is always better to look at the weight function directly.
  -->
  <class type="fuse_loss::ArctanLoss" base_class_type="fuse_core::Loss">
    <description>
    Loss function that is capped beyond a certain level using the arc-tangent function, with scaling parameter 'a' that
    determines the level where falloff occurs. For costs much smaller than 'a', the loss function is linear and behaves
    like TrivialLoss, and for values much larger than 'a' the value asymptotically approaches the constant value of
    'a * PI / 2'. It is defined as follows for the squared residual 's':

    rho(s) = a * atan2(s, a)
    </description>
  </class>
  <class type="fuse_loss::CauchyLoss" base_class_type="fuse_core::Loss">
    <description>
    Loss function inspired by the Cauchy distribution, with scaling parameter 'a', defined as follows for the squared
    residual 's':

    rho(s) = b * log(1 + s * c)

    where b = a^2 and c = 1 / b.
    </description>
  </class>
  <class type="fuse_loss::ComposedLoss" base_class_type="fuse_core::Loss">
    <description>
    Composition of two loss functions. The error is the result of first evaluating 'g' followed by 'f' to yeild the
    composition 'f(g(s))' for the squared residual 's':

    rho(s) = f_rho(g_rho(s))

    If either the 'f' or 'g' loss functions are not given, the Trivial loss is used by default.
    </description>
  </class>
  <class type="fuse_loss::DCSLoss" base_class_type="fuse_core::Loss">
    <description>
    DCS (Dynamic Covariance Scaling) loss function with scaling parameter 'a', defined as follows for the squared
    residual 's':

    rho(s) = a * (3 * s - a) / (a + s)  if s > a   // outlier region
           = s                          otherwise  // inlier region

    which weight function:

    rho'(s) = min { 1, (2 * a / (a + s))^2 }

    as described in Eq. 5.19 and 5.20 in:

      http://www2.informatik.uni-freiburg.de/~agarwal/resources/agarwal-thesis.pdf (p. 89)

    that is equal to the square of the scaling factor in Eq. 15 in the original paper:

      http://www2.informatik.uni-freiburg.de/~spinello/agarwalICRA13.pdf (p.3)
    </description>
  </class>
  <class type="fuse_loss::FairLoss" base_class_type="fuse_core::Loss">
    <description>
    Fair loss function with scaling parameter 'a', defined as follows for the squared residual 's':

    rho(s) = 2 * b * (r/a - log(1 + r/a))

    where b = a^2 and r = sqrt(s).
    </description>
  </class>
  <class type="fuse_loss::GemanMcClureLoss" base_class_type="fuse_core::Loss">
    <description>
    Geman-McClure loss function with scaling parameter 'a', defined as follows for the squared residual 's':

    rho(s) = s * b / (b + s)

    where b = a^2.
    </description>
  </class>
  <class type="fuse_loss::HuberLoss" base_class_type="fuse_core::Loss">
    <description>
    Huber loss function with scaling parameter 'a', defined as follows for the squared residual 's':

    rho(s) = 2 * a * sqrt(s) - b  if s > b   // outlier region
           = s                    otherwise  // inlier region

    where b = a^2.
    </description>
  </class>
  <class type="fuse_loss::ScaledLoss" base_class_type="fuse_core::Loss">
    <description>
    The other basic loss function has to do with length scaling, i.e. they affect the space in which 's' is measured.
    Sometimes you want to simply scale the output value of the robustifier.

    If '\hat{rho}' is the wrapped robustifier, then this simply outputs:

    rho(s) = a * \hat{rho(s)}

    If '\hat{rho}' is not given, it is set as NULL and is treated as the Trivial loss function, so we get:

    rho(s) = a * s
    </description>
  </class>
  <class type="fuse_loss::SoftLOneLoss" base_class_type="fuse_core::Loss">
    <description>
    Soft L1 loss function with scaling parameter 'a', that is similar to Huber but smooth, defined as follows for the
    squared residual 's':

    rho(s) = 2 * b * (sqrt(1 + s * c) - 1)

    where b = a^2 and c = 1 / b.

    This is call Pseudo-Huber in: https://en.wikipedia.org/wiki/Huber_loss#Pseudo-Huber_loss_function
    </description>
  </class>
  <class type="fuse_loss::TolerantLoss" base_class_type="fuse_core::Loss">
    <description>
    Loss function that maps to approximately zero cost in a range around the origin, and reverts to linear in error
    (quadratic in cost) beyond this range. The tolerance parameter 'a' sets the nominal point at which the transition
    occurs, and the transition size parameter 'b' sets the nominal distance over which most of the transition occurs.
    Both 'a' and 'b' must be greater than zero, and typically 'b' will be set to a fraction of 'a'. It is defined as
    follows for the squared residual 's':

    rho(s) = b * log(1 + exp((s - a) / b)) - c0

    where c0 = b * log(1 + exp(-a / b)).
    </description>
  </class>
  <class type="fuse_loss::TrivialLoss" base_class_type="fuse_core::Loss">
    <description>
    This corresponds to no robustification. It is not normally necessary to use this, as not not setting a loss function
    accomplishes the same thing.

    rho(s) = s
    </description>
  </class>
  <class type="fuse_loss::TukeyLoss" base_class_type="fuse_core::Loss">
    <description>
    Tukey biweight loss function which aggressively attempts to suppress large errors, with scaling parameter 'a',
    defined as follows for the squared residual 's':

    rho(s) = a^2 / 6                          if s > a^2  // inlier region
           = a^2 / 6 * (1 - (1 - s / a^2)^3)  otherwise   // outlier region
    </description>
  </class>
  <class type="fuse_loss::WelschLoss" base_class_type="fuse_core::Loss">
    <description>
    Welsch loss function with scaling parameter 'a', defined as follows for the squared residual 's':

    rho(s) = b * (1 - exp(-s/b))

    where b = a^2.
    </description>
  </class>
</library>
